<!DOCTYPE html>
<html>
<head>
  <title>Redes Neuronales: Un modelo computacional inspirado en el funcionamiento del cerebro humano</title>
<style>
  body {
    max-width: 800px;
    margin: 0 auto;
    padding: 50px;
    font-family: Arial, sans-serif;
    background-color: #f8f8f8;
  }

  h1 {
    text-align: center;
    font-size: 28px;
    color: #333;
    margin-bottom: 20px;
    text-transform: uppercase;
  }

  p {
    font-size: 18px;
    line-height: 1.6;
    color: #555;
    margin-bottom: 10px;
    text-align: justify;
  }

  ul {
    list-style-type: disc;
    margin-left: 20px;
    margin-bottom: 10px;
  }

  li {
    font-size: 18px;
    line-height: 1.6;
    color: #555;
  }
</style>
 
</head>
<body>
  <h1>Redes Neuronales: Un modelo computacional inspirado en el funcionamiento del cerebro humano</h1>
  <p>Autor: Julian Villaseñor Ibarra</p>
  <p>Maestría en Ciencia de Datos</p>
  <p>Una red neuronal es un modelo computacional inspirado en el funcionamiento del cerebro humano. Está compuesta por un conjunto interconectado de unidades básicas llamadas neuronas artificiales o nodos, que trabajan en conjunto para resolver problemas de manera paralela y aprender a partir de los datos.</p>
  <p>Cada neurona artificial en una red neuronal está conectada a otras neuronas mediante enlaces o conexiones ponderadas. Estos enlaces representan la fuerza de la conexión entre las neuronas y se utilizan para transmitir señales entre ellas. Cada neurona recibe una o varias entradas, las procesa mediante una función de activación y produce una salida que puede ser enviada como entrada a otras neuronas.</p>
  <p>Las redes neuronales pueden tener diferentes arquitecturas, pero una de las más comunes es la red neuronal feedforward o red neuronal de alimentación directa. En este tipo de red, las señales fluyen en una sola dirección, desde las capas de entrada, pasando por una o varias capas ocultas, hasta llegar a la capa de salida.</p>
  
  <p>El funcionamiento de una red neuronal se basa en principios matemáticos fundamentales. Para comprender cómo las redes neuronales toman decisiones y procesan la información, es importante conocer los conceptos matemáticos.</p>
  
 


  <h2>Algunos ejemplos de aplicaciones de redes neuronales:</h2>
  <ul>
    <li>Reconocimiento de imágenes: Las redes neuronales se utilizan para reconocer objetos, caras o patrones en imágenes. Por ejemplo, se pueden entrenar redes neuronales para reconocer gatos en fotografías.</li>
    <li>Procesamiento del lenguaje natural: Las redes neuronales se utilizan en tareas como traducción automática, análisis de sentimientos, generación de texto, entre otros. Por ejemplo, los chatbots y los asistentes virtuales utilizan redes neuronales para entender y responder a las preguntas en lenguaje natural.</li>
    <li>Pronóstico y predicción: Las redes neuronales se pueden utilizar para predecir valores futuros en series de tiempo, como el precio de las acciones, la demanda de productos, el clima, entre otros.</li>
    <li>Reconocimiento de voz: Las redes neuronales se emplean en sistemas de reconocimiento de voz para convertir el habla en texto o para identificar comandos de voz en dispositivos inteligentes.</li>
    <li>Conducción autónoma: Las redes neuronales se utilizan en vehículos autónomos para reconocer objetos en tiempo real, tomar decisiones de conducción y controlar los sistemas de navegación.</li>
  </ul>
  <h2>Esquema general que involucra las redes neuronales:</h2>
  <ol>
    <li>Recopilación y preparación de datos: En esta etapa, se recopilan los datos relevantes para el problema que se desea resolver. Los datos pueden provenir de diferentes fuentes, como bases de datos, archivos, sensores, etc. Además, es necesario preparar los datos para que estén en un formato adecuado para su procesamiento por una red neuronal. Esto puede incluir la limpieza de datos, la normalización, la codificación de variables categóricas, etc.</li>
    <li>Diseño de la arquitectura de la red: En esta etapa, se define la estructura de la red neuronal, incluyendo el número de capas, la cantidad de neuronas en cada capa, la función de activación, etc. Esto depende en gran medida del tipo de problema que se está abordando y los datos disponibles.</li>
    <li>Entrenamiento de la red: En esta etapa, se utiliza un algoritmo de aprendizaje para ajustar los parámetros de la red neuronal. Se presenta a la red los datos de entrenamiento junto con las salidas esperadas, y la red ajusta sus conexiones ponderadas para minimizar la diferencia entre las salidas predichas y las salidas esperadas. Este proceso se repite iterativamente hasta que la red alcanza un nivel satisfactorio de precisión en la tarea.</li>
    <li>Validación y ajuste: Después del entrenamiento, se utiliza un conjunto de datos separado, llamado conjunto de validación, para evaluar el rendimiento de la red neuronal. Esto permite ajustar los hiperparámetros de la red, como la tasa de aprendizaje o el número de capas ocultas, con el fin de mejorar el rendimiento.</li>
    <li>Evaluación y predicción: Una vez que la red neuronal ha sido entrenada y ajustada, se puede utilizar para hacer predicciones o clasificaciones en nuevos datos. La red recibe las entradas y genera salidas basadas en los patrones aprendidos durante el entrenamiento.</li>
  </ol>
 <p>Problema de interes: Prediccion de Bitcoin</p>
<p>Para predesir los precios de la crypto moneda Bitcoin basados en un conjunto de datos, usare el modelo de Redes Neuronales Recurrente, las cuales estan especificamente diseñadas para el analisis de tiempo. Las redes neuronales recurrentes son adecuadas para modelar dependencias secuenciales y son ampliamente utilizadas en este titpo de problemas de prediccion de series de tiempo.</p>
  <p>Dentro del las Redes neuronales recurrentes, estan la redes neuronales LSTM que en ingles dice Long Short-Term Memory, las cuales son muy efectivas en el modelado de secuencaias a largo plazo y evitan el problema del desvanecimiento del gradiente. La arquitectura basica de una red neuronal LSTM incluyen celdas de memoria que permiten recordar informacion a largo plazo y puertas de entrada, salida y olvido que controlan el flujo de informacion en la red</p>
<p>Ejemplo de código de una red neuronal LSTM:</p>
<pre>
<code>
from keras.models import Sequential
from keras.layers import LSTM, Dense

model = Sequential()
model.add(LSTM(units=64, input_shape=(timesteps, features)))
model.add(Dense(units=1))

model.compile(optimizer='adam', loss='mean_squared_error')
model.fit(X_train, y_train, epochs=10, batch_size=32)
<p>El proceso matemático para este tipo de red neuronal es:</p>

<p>Claro, a continuación te mostraré el proceso matemático de una red neuronal recurrente 

<head>
    <title>LaTeX en HTML</title>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>
</head>


(RNN) en formato LaTeX:</p>

<p>Inicialización de parámetros:</p>
<p>\( \begin{align*} W_{hh}^{(0)}, W_{xh}^{(0)}, b_h^{(0)} & \text{ - Pesos y sesgos de la capa oculta en el instante 0} \\ W_{hy}^{(0)}, b_y^{(0)} & \text{ - Pesos y sesgo de la capa de salida en el instante 0} \end{align*} \)</p>

<p>Propagación hacia adelante:</p>
<p>\( \begin{align*} h_t &= f(W_{hh}^{(t-1)} \cdot h_{t-1} + W_{xh}^{(t-1)} \cdot x_t + b_h^{(t-1)}) \\ y_t &= g(W_{hy}^{(t-1)} \cdot h_t + b_y^{(t-1)}) \end{align*} \)</p>
<p>donde:</p>
<p>\( h_t \) es el estado oculto en el instante \( t \)</p>
<p>\( x_t \) es la entrada en el instante \( t \)</p>
<p>\( y_t \) es la salida en el instante \( t \)</p>
<p>\( f(\cdot) \) es la función de activación de la capa oculta</p>
<p>\( g(\cdot) \) es la función de activación de la capa de salida</p>

<p>Cálculo del error:</p>
<p>\( E_t = \frac{1}{2}(y_t - \hat{y}_t)^2 \)</p>
<p>donde \( \hat{y}_t \) es el valor objetivo en el instante \( t \)</p>

<p>Retropropagación del error:</p>
<p>\( \begin{align*} \delta_y^{(t)} &= (y_t - \hat{y}t) \cdot g'(net_y^{(t)}) \\ \delta_h^{(t)} &= (\delta_y^{(t)} \cdot W_{hy}^{(t)}) \odot f'(net_h^{(t)}) \end{align*} \)</p>
<p>donde \( \odot \) representa la multiplicación elemento a elemento y \( net_y^{(t)} \) y \( net_h^{(t)} \) son las entradas ponderadas de las capas de salida y oculta en el instante \( t \), respectivamente.</p>

<p>Actualización de pesos y sesgos:</p>
<p>\( \begin{align*} W_{hy}^{(t)} &\leftarrow W_{hy}^{(t-1)} - \eta \cdot \delta_y^{(t)} \cdot h_t \\ b_y^{(t)} &\leftarrow b_y^{(t-1)} - \eta \cdot \delta_y^{(t)} \\ W_{hh}^{(t)} &\leftarrow W_{hh}^{(t-1)} - \eta \cdot \

</code>
</pre>

</body>
</html>
